{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocessing.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"j_MwdRYy6Ex9"},"source":["# Configuration"]},{"cell_type":"code","metadata":{"id":"mbXQ_SA56FCm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608644400247,"user_tz":-60,"elapsed":18088,"user":{"displayName":"Christian Bernasconi","photoUrl":"","userId":"18091163232251146866"}},"outputId":"8aef802f-e726-4e4c-cbd4-637908be829d"},"source":["# set main directories\r\n","BASE_DIR = '/content/gdrive/MyDrive/AML_project/project/'\r\n","UTILS_DIR = BASE_DIR + 'utils'\r\n","DATA_DIR = BASE_DIR + 'data/'\r\n","TOKENIZER_DIR = BASE_DIR + 'tokenizers/'\r\n","# mount drive\r\n","from google.colab import drive\r\n","drive.mount('/content/gdrive', force_remount=True)\r\n","# set utils directory\r\n","import sys\r\n","sys.path.append(UTILS_DIR)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tvqPLMkb3VCo"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"27-z_Bsy2_G4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608644409045,"user_tz":-60,"elapsed":26881,"user":{"displayName":"Christian Bernasconi","photoUrl":"","userId":"18091163232251146866"}},"outputId":"7079f240-d026-420a-fe6d-4d3c79039ed3"},"source":["import numpy as np\r\n","import random as rnd\r\n","import pandas as pd\r\n","import nltk\r\n","import string\r\n","import unicodedata\r\n","import pickle\r\n","import re\r\n","import io\r\n","from nltk import word_tokenize, pos_tag\r\n","from nltk.corpus import stopwords, wordnet\r\n","\r\n","from keras.preprocessing.text import Tokenizer\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","# custom imports\r\n","from utils import getComments\r\n","from contracted_forms import contractions\r\n","from emoticons import emoticons_replaces\r\n","from preprocessing import Preprocessor"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qfGK5L7b_vQ6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608644410442,"user_tz":-60,"elapsed":28268,"user":{"displayName":"Christian Bernasconi","photoUrl":"","userId":"18091163232251146866"}},"outputId":"d1a0e89e-0333-4ff4-c5fc-b384a64626b0"},"source":["nltk.download('stopwords')\r\n","nltk.download('punkt')\r\n","nltk.download('wordnet')\r\n","nltk.download('averaged_perceptron_tagger')\r\n","nltk.download('tagsets')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package tagsets to /root/nltk_data...\n","[nltk_data]   Unzipping help/tagsets.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"XRZ-BCzSVAhW"},"source":["# Load data"]},{"cell_type":"markdown","metadata":{"id":"U_TcShQJma9l"},"source":["## Original data"]},{"cell_type":"code","metadata":{"id":"dWKcf0qbU-p_"},"source":["# load train\r\n","train = pd.read_csv(DATA_DIR + 'train.csv')\r\n","\r\n","# load test\r\n","test = pd.read_csv(DATA_DIR + 'test.csv')\r\n","\r\n","# load test labels\r\n","test_labels = pd.read_csv(DATA_DIR + 'test_labels.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pGaRvJJgNJeg"},"source":["## Data augmentation with back translations"]},{"cell_type":"markdown","metadata":{"id":"Du1zmLQx8vlU"},"source":["Load DE, FR and ES back translation to EN"]},{"cell_type":"code","metadata":{"id":"yLsMEM6TNlxl"},"source":["# load back translated comments\n","train_de = pd.read_csv(DATA_DIR + 'train_de.csv')\n","train_fr = pd.read_csv(DATA_DIR + 'train_fr.csv')\n","train_es = pd.read_csv(DATA_DIR + 'train_es.csv')\n","# select ONLY not clean comments to reduce the imbalance\n","train_de = getComments(train_de, clean=False)\n","train_fr = getComments(train_fr, clean=False)\n","train_es = getComments(train_es, clean=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2dFyLlXsC5PL"},"source":["Add a label to identify each source."]},{"cell_type":"code","metadata":{"id":"XTiKBOKyC4i1"},"source":["train['source'] = 'en'\r\n","train_de['source'] = 'de'\r\n","train_fr['source'] = 'fr'\r\n","train_es['source'] = 'es'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UxHCzvsuDl7T"},"source":["Augment original data"]},{"cell_type":"code","metadata":{"id":"6X4fh8aaDpOk"},"source":["train = train.append([train_de, train_fr, train_es], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N8t_jalpmN1O"},"source":["## Remove invalid data"]},{"cell_type":"code","metadata":{"id":"0sgkD_K0Xy91"},"source":["train['comment_text'] = train['comment_text'].astype(str)\r\n","\r\n","# remove empty comments\r\n","train.drop(index = train[train['comment_text'] == ''].index, inplace=True)\r\n","\r\n","test['comment_text'] = test['comment_text'].astype(str)\r\n","\r\n","# remove empty comments\r\n","test.drop(index = test[test['comment_text'] == ''].index, inplace=True)\r\n","\r\n","# Remove invalid test instances\r\n","indexes = test_labels[test_labels['toxic'] == -1].index\r\n","test_labels.drop(index=indexes, inplace=True)\r\n","test.drop(index=indexes, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E2ao4zK1VVcj"},"source":["# Preprocess text"]},{"cell_type":"markdown","metadata":{"id":"7lEMB4WoXJnG"},"source":["#### Instantiate a Preprocessor object responsible of all the preprocessing steps"]},{"cell_type":"code","metadata":{"id":"NNsPaUjGVYQW"},"source":["p = Preprocessor(keep_punct='!', replace_emojis=False, correct_bad_words=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C3bxeE1VXCyn"},"source":["### Clean text in train"]},{"cell_type":"code","metadata":{"id":"5KjFmdfyVOaS"},"source":["# preprocess train\r\n","train['comment_text'] = train['comment_text'].map(p.preprocessDoc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UpHBQZ7ivYDX"},"source":["# remove invalid texts after cleaning\r\n","ids = train[train['comment_text'] == '']['id']\r\n","train = train[~train['id'].isin(ids)]\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K6bZvNM7XFHs"},"source":["### Clean text in test"]},{"cell_type":"code","metadata":{"id":"16eLhsx-XA7H"},"source":["test['comment_text'] = test['comment_text'].map(p.preprocessDoc)\r\n","\r\n","# remove invalid texts after cleaning\r\n","indexes = test[test['comment_text'] == ''].index\r\n","test.drop(index = indexes, inplace=True)\r\n","test_labels.drop(index=indexes, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AmrrqJkBzvuU"},"source":["### Remove useless columns"]},{"cell_type":"code","metadata":{"id":"0eFminDXz0k0"},"source":["train.drop(labels=['id'], axis=1, inplace=True)\r\n","test.drop(labels=['id'], axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QfnUgqPaibNG"},"source":["# Save Tokenizer for future preprocessings"]},{"cell_type":"code","metadata":{"id":"JB2XJ9_1ignX"},"source":["p.saveTokenizer(TOKENIZER_DIR + 'base_tokenizer.pickle')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pbO6JB4cm49-"},"source":["# Data augmentation with synonyms"]},{"cell_type":"markdown","metadata":{"id":"SOh3hf_9p4OV"},"source":["Generate new comments replacing words with synonyms"]},{"cell_type":"code","metadata":{"id":"80LBXIEAoBIs"},"source":["# select ONLY original language comments\r\n","train_en = train[train['source'] == 'en']\r\n","# select ONLY toxic comments to reduce the imbalance\r\n","train_en = getComments(train_en, clean=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DI8Q62W1Ch-f"},"source":["dict_pos = {'NN': wordnet.NOUN, 'JJ': wordnet.ADJ, 'VB': wordnet.VERB}\r\n","\r\n","# get a synonym of a word depending to the part of speach\r\n","def getSynonym(word, pos):\r\n","  syns = []\r\n","  # iterate over all possible synonyms\r\n","  for syn in wordnet.synsets(word):\r\n","    # check that words represent the same POS\r\n","    if syn.pos() == dict_pos[pos]:\r\n","      # iterate over all possible lemmas\r\n","      for lemma in syn.lemmas():\r\n","        syns.append(lemma.name().replace('_', ' ').replace('-', ' '))\r\n","  # pick a random synonym      \r\n","  if len(syns)>0:\r\n","    syn = rnd.choice(syns)\r\n","  else:\r\n","    syn = word\r\n","  return syn\r\n","\r\n","# replace synonyms in a comment given a replacement probability\r\n","def replaceSynonyms(comment, replace_probabilty = 1):\r\n","  poss = pos_tag(word_tokenize(comment))\r\n","  new_comment = []\r\n","  for pos in poss:\r\n","    if rnd.random() < replace_probabilty:\r\n","      w = pos[0]\r\n","      p = pos[1][0:2]\r\n","      if (p=='NN') | (p=='JJ') | (p=='VB') :\r\n","        w = getSynonym(w, p).lower()\r\n","      new_comment.append(w)\r\n","  return ' '.join(new_comment)\r\n","\r\n","# create a new datafram by substituting words with their synonyms\r\n","def augmentWithSynonyms(X, n_times = 1, replace_probability = 1):\r\n","  total_documents = len(X)*n_times\r\n","  count = 0\r\n","  X_syn = pd.DataFrame(columns=X.columns)\r\n","  for i in range(n_times):\r\n","    for index, row in X.iterrows():\r\n","      count = count+1\r\n","      print('\\r' + '{0}/{1} documents'.format(count, total_documents), end='')\r\n","      row['comment_text'] = replaceSynonyms(row['comment_text'], replace_probability)\r\n","      row['source'] = 'syn_' + str(i)\r\n","      X_syn = X_syn.append(row, ignore_index=True)\r\n","  return X_syn\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4vQhI8r-ypT6","executionInfo":{"status":"ok","timestamp":1608645361485,"user_tz":-60,"elapsed":979261,"user":{"displayName":"Christian Bernasconi","photoUrl":"","userId":"18091163232251146866"}},"outputId":"c940625a-1480-4b2a-be6c-49e1bc32e176"},"source":["# triplicate toxic comments with synonyms\r\n","train_syn = augmentWithSynonyms(train_en, n_times=3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["48636/48636 documents"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LCiCcrZGM8Bv"},"source":["# augment original data\r\n","train = train.append(train_syn, ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r_tneJCJSfzQ"},"source":["# Add generic 'is_toxic' label"]},{"cell_type":"markdown","metadata":{"id":"nD_DSRNyORKb"},"source":["Set \"is_toxic\" equal to 1 in order to use it in a binary classifier of toxic comments"]},{"cell_type":"code","metadata":{"id":"c9mohjaCSfCo"},"source":["# init is_toxic\r\n","train['is_toxic'] = 0\r\n","test_labels['is_toxic'] = 0\r\n","# assign 1 to is_toxic of all the toxic comments\r\n","toxic_indexes = getComments(train, clean=False).index\r\n","train.loc[toxic_indexes, 'is_toxic'] = 1\r\n","toxic_indexes = getComments(test_labels, clean=False).index\r\n","test_labels.loc[toxic_indexes, 'is_toxic'] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"USUwumrtLcYr"},"source":["# Generate undersample of not toxic comments"]},{"cell_type":"markdown","metadata":{"id":"OTVFm2PZCTBQ"},"source":["Basic undersample of the clean comments."]},{"cell_type":"code","metadata":{"id":"YPiTo9KZdD2K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608645362023,"user_tz":-60,"elapsed":979789,"user":{"displayName":"Christian Bernasconi","photoUrl":"","userId":"18091163232251146866"}},"outputId":"13d70dea-da65-4622-ece0-6ca7cefa748f"},"source":["train_clean = getComments(train, clean=True)\n","train_toxic = getComments(train, clean=False)\n","print('Number of clean comments: ', len(train_clean))\n","print('Number of not clean comments: ', len(train_toxic))\n","train_clean = train_clean.sample(n=90000, random_state=42)\n","train_undersample = pd.concat([train_clean, train_toxic], axis=0, ignore_index=True)\n","print('Clean comments undersampled to ', 90000)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of clean comments:  143261\n","Number of not clean comments:  113484\n","Clean comments undersampled to  90000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rFzjr_jxfXSE"},"source":["# Save cleaned data"]},{"cell_type":"code","metadata":{"id":"XvXFd6Yze8qN"},"source":["# save augmented data to csv\r\n","train.to_csv(DATA_DIR + 'train_aug_cleaned.csv', index=False, sep=\"\\t\")\r\n","test.to_csv(DATA_DIR + 'test_cleaned.csv', index=False, sep=\"\\t\")\r\n","test_labels.to_csv(DATA_DIR + 'test_labels_cleaned.csv', index=False, sep=\"\\t\")\r\n","# save augmented data + not toxic undersample to csv\r\n","train_undersample.to_csv(DATA_DIR + 'train_aug_und_cleaned.csv', index=False, sep=\"\\t\")"],"execution_count":null,"outputs":[]}]}